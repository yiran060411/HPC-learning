# 吴恩达机器学习（四）

以下是根据文档内容整理的简洁中文笔记，结合专业概念解释，结构清晰易读：

---

### **神经网络学习：手写数字识别**

**任务目标**：实现反向传播算法训练神经网络，识别20×20像素的手写数字（0-9）。

---

### **一、核心概念**

1. **数据集**
    - 5000个训练样本（`ex4data1.mat`）
    - 每张图片展开为400维向量（20×20像素）
    - 标签 `y`：数字0标记为10，1-9按原值标记
2. **神经网络结构**（图2）
    
    ![](https://hunyuan-plugin-private-1258344706.cos.ap-nanjing.myqcloud.com/pdf_youtu/img/c5ce6c43f40f88ebed37f8a96d70ee97-image.png?q-sign-algorithm=sha1&q-ak=AKID372nLgqocp7HZjfQzNcyGOMTN3Xp6FEA&q-sign-time=1753109447%3B2068469447&q-key-time=1753109447%3B2068469447&q-header-list=host&q-url-param-list=&q-signature=b05a6aa46ad4380571b49f9e60d6f00d0feb1b98)
    
    - **输入层**：400单元（像素值）+ 1偏置单元
    - **隐藏层**：25单元（使用预训练参数 `ex4weights.mat`）
    - **输出层**：10单元（对应数字0-9）
3. **标签向量化**（One-hot编码）
    - 例：数字5 → 输出向量 $[0,0,0,0,1,0,0,0,0,0]$

---

### **二、关键算法**

1. **代价函数**（正则化版本）
    
    J$(\theta) = \frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K} \left[ -y_k^{(i)} \log(h_\theta(x^{(i)})_k) - (1-y_k^{(i)}) \log(1 - h_\theta(x^{(i)})_k) \right] + \frac{\lambda}{2m} \left( \sum \Theta^2 \right)$
    
    - $hθ(x)$：神经网络输出概率
    - 正则化项（λ）防止过拟合（忽略偏置单元权重）
2. **反向传播步骤**（图3）
    
    ![](https://hunyuan-plugin-private-1258344706.cos.ap-nanjing.myqcloud.com/pdf_youtu/img/49d1262ed91bea9ce302b04b54f359a2-image.png?q-sign-algorithm=sha1&q-ak=AKID372nLgqocp7HZjfQzNcyGOMTN3Xp6FEA&q-sign-time=1753109459%3B2068469459&q-key-time=1753109459%3B2068469459&q-header-list=host&q-url-param-list=&q-signature=d11f8291b904be59ede431c66e94aba151abc473)
    
    ```
    for 每个样本 i:
      1. 前向传播计算各层激活值 (a⁽¹⁾, a⁽²⁾, a⁽³⁾)
      2. 计算输出层误差 δ⁽³⁾ = a⁽³⁾ - y
      3. 计算隐藏层误差 δ⁽²⁾ = (Θ⁽²⁾)ᵀδ⁽³⁾ * g'(z⁽²⁾)
      4. 累积梯度 Δ += δ⁽ˡ⁺¹⁾(a⁽ˡ⁾)ᵀ
      5. 正则化梯度：D⁽ˡ⁾ = Δ/m + (λ/m)Θ⁽ˡ⁾ (j≥1时)
    ```
    
3. **Sigmoid梯度**
    
    $g'(z) = g(z)(1 - g(z))$
    
    - 代码文件：`sigmoidGradient.m`

---

### **三、实现细节**

1. **参数初始化**（`randInitializeWeights.m`）
    - 权重随机生成在 $[-ε, ε]$ 区间（ε=0.12），打破对称性
2. **梯度验证**（`checkNNGradients.m`）
    - 数值梯度近似公式：$f_i(\theta) \approx \frac{J(\theta^{(i+)}) - J(\theta^{(i-)})}{2\epsilon}$
    - 误差需 < 10⁻⁹
3. **训练流程**
    - 使用优化器 `fmincg` 最小化代价函数
    - 成功训练后准确率 ≈95.3%（可提升至100%）

---

### **四、结果可视化**

1. **隐藏层单元可视化**（图4）
    
    ![](https://hunyuan-plugin-private-1258344706.cos.ap-nanjing.myqcloud.com/pdf_youtu/img/59f08261758af396bf369b7e99aed4b1-image.png?q-sign-algorithm=sha1&q-ak=AKID372nLgqocp7HZjfQzNcyGOMTN3Xp6FEA&q-sign-time=1753109474%3B2068469474&q-key-time=1753109474%3B2068469474&q-header-list=host&q-url-param-list=&q-signature=c2d3826f1744bcfa5bff212ad112c67ae6ccc4fe)
    
    - 每个隐藏单元权重转换为20×20图像
    - 白色区域：激活该单元的笔画模式

---

### **五、优化建议**

1. **超参数调优**
    - **λ（正则化强度）**：过高导致欠拟合，过低导致过拟合
    - **迭代次数**：增加 `MaxIter`（如400次）提升准确率
2. **实践提示**
    - 梯度检查仅在调试时开启，训练时关闭
    - 隐藏层单元数可调整（文档中固定为25）

---

**附：文件清单**

- 必写代码：`nnCostFunction.m`（代价函数+梯度）
- 辅助函数：`sigmoidGradient.m`、`randInitializeWeights.m`

> 此笔记保留原文档逻辑，用中文重述核心内容，省略冗余描述与代码细节。专业术语（如One-hot编码、正则化）已融入上下文解释，无需额外注释。
>