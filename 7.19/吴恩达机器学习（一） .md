# 吴恩达机器学习（一）

# **线性回归与利润预测实操**

## **1. 机器学习的定义**

机器学习的**定义**是让计算机能够在没有明确编程的情况下自主学习。吴恩达引用了 Arthur Samuel 的经典定义：“**机器学习**是这样一个领域，它赋予计算机无需显式编程就能学习的能力”。更严格地说，Tom Mitchell 提出：“一个程序被认为能从经验E中学习，针对某类任务T和性能度量P，如果它在任务T上的性能（由P度量）随着经验E的增加而提升”。简单来说，机器学习就是**让算法从数据中学习规律**，从而提高预测或决策的准确性。

例如，垃圾邮件分类就是机器学习的典型应用：程序通过分析大量邮件（经验E）学会区分垃圾邮件和正常邮件（任务T），随着分析的邮件越多，分类的准确率（性能P）就越高。

## **2. 监督学习**

**监督学习（Supervised Learning）**是指从**有标签的训练数据**中学习规律，以预测新数据的输出。在监督学习中，我们给算法提供一组**输入-输出对（x, y）**，算法需要从中学习一个函数，将输入映射到输出。监督学习的目标是，当遇到新的输入时，能够**预测相应的输出**。

监督学习主要分为两种：**回归（Regression）**和**分类（Classification）**：

- **回归问题**：预测的是一个**连续的数值**。例如，根据房屋面积预测房价，输出是一个具体的数值。
- **分类问题**：预测的是一个**离散的类别**。例如，根据肿瘤特征判断肿瘤是良性还是恶性，输出是类别标签（良性/恶性）。

吴恩达课程中用了两个例子来说明：

- **房价预测**：给定一组房屋面积和价格的数据（输入是面积，输出是价格），学习一个模型来预测任意面积房屋的价格。这是一个回归问题，因为价格是连续值。
- **肿瘤分类**：给定一组肿瘤特征和诊断结果（输入是肿瘤的各项特征，输出是“恶性”或“良性”），学习一个模型来预测新肿瘤是良性还是恶性。这是一个分类问题，因为输出是离散的类别。

监督学习的关键在于**训练数据带有正确答案（标签）**，算法通过不断调整模型去拟合这些已知的输入-输出对，从而能够对未知数据进行预测。

## **3. 无监督学习**

**无监督学习（Unsupervised Learning）**是指从**无标签的数据**中发现隐含的结构或模式。在无监督学习中，我们只提供输入数据，没有对应的输出标签，算法需要自行**找出数据中的规律或分组**。简而言之，就是让算法**“在数据中找到隐藏的结构”**。

无监督学习的一个典型例子是**聚类（Clustering）**算法。例如，给定一组未标记的新闻文章，聚类算法可以自动将它们分成若干主题组，即使我们事先并不知道这些主题是什么。再如，在社交网络分析中，可以通过无监督学习发现**朋友圈**：根据用户互动数据自动将用户分组，每组形成一个社交圈，而不需要预先指定这些分组。

吴恩达课程中提到的**鸡尾酒会问题**也是一个有趣的例子：在嘈杂的鸡尾酒会环境中，有两个人同时说话，我们需要从混合的录音中分离出每个人的声音。通过无监督学习算法（如独立成分分析），可以在**没有任何标签**的情况下，自动将两种声音分离开来。

总的来说，无监督学习适用于**探索性分析**，帮助我们从大量无结构的数据中发现潜在的模式或分组，例如市场细分、社交网络社区发现、基因序列分析等。

## **4. 模型表示**

在监督学习中，我们通常用**假设函数（hypothesis）**来表示模型对输入到输出映射的预测。假设函数通常写作 $h_θ(x)$，其中 $x$ 是输入特征，$θ$ 是模型的参数（权重和偏置）。对于**线性回归**模型，假设函数是一个线性组合，形式为：

$h_θ(x) = θ₀ + θ₁x₁ + θ₂x₂ + ... + θₙxₙ$

其中 $x₁, x₂, ..., xₙ$ 是输入的各个特征，$θ₀, θ₁, ..., θₙ$ 是模型需要学习的参数（$θ₀$ 称为偏置项，对应直线的截距）。如果只有一个特征（一元线性回归），上式简化为 $h_θ(x) = θ₀ + θ₁x$，这就是一条直线的方程。

我们的目标是通过训练数据来**学习参数**$θ$的值，使得对于输入x，预测值$h_θ(x)$尽可能接近真实值y。换句话说，就是找到一条“最佳”直线（或超平面）来拟合训练数据。

例如，在房价预测问题中，假设我们只用房屋面积一个特征（$x$）来预测价格（$y$），那么模型可以表示为 $h_θ(x) = θ₀ + θ₁x$。我们需要学习参数$θ₀$和$θ₁$，使得对于训练集中的房屋面积x，预测的价格$h_θ(x)$与真实价格y尽可能接近。

模型表示的关键在于**选择合适的函数形式**（这里我们选择了线性函数）并确定其中的参数。不同的模型形式（线性、多项式、非线性等）会影响模型的表达能力和训练方法，但线性模型是最基础且应用广泛的形式之一。

## **5. 代价函数**

**代价函数（Cost Function）**是用于衡量模型预测值与真实值之间误差的函数。在机器学习中，我们通常希望**最小化**这个误差，因此代价函数有时也被称为**损失函数（Loss Function）**或**目标函数（Objective Function）**。简单来说，代价函数告诉我们模型当前的表现有多差，我们的目标就是通过调整模型参数来降低这个代价函数的值。

对于线性回归问题，最常用的代价函数是**均方误差（Mean Squared Error, MSE）**。

均方误差的计算方法是：对每个训练样本，计算模型预测值与真实值之差的平方，然后将所有样本的平方误差求平均（或求和）。数学上，对于有m个训练样本的情况，代价函数$J(θ)$可以表示为：

$J(θ) = (1/(2m)) * Σ (h_θ(x^i) - y^i)^2$

其中，$(x^i, y^i)$表示第i个训练样本，$h_θ(x^i)$是模型对第i个样本的预测值，$y^i$是真实值。系数$1/(2m)$是为了计算方便（梯度下降时求导后系数为1/m）。

**直观理解**：代价函数相当于在参数空间中形成一个**“碗”形曲面**，碗的最低点对应最小的误差值。我们的目标就是找到使$J(θ)$最小的参数$θ$。如果我们把参数$θ₀$和$θ₁$作为坐标轴，画出$J(θ)$的等高线图，那么每一条等高线代表相同代价的参数组合，中心处就是代价最小的点。

例如，在单变量线性回归中，如果我们尝试不同的$θ₀$和$θ₁$值并计算对应的$J(θ)$，可以画出一个曲面或等高线图。当$θ₀$和$θ₁$取到最佳拟合直线的参数时，$J(θ)$会达到最小值，对应等高线图的中心。

通过最小化代价函数，我们可以找到**最佳拟合**训练数据的模型参数。这是监督学习中训练模型的核心思想：定义一个衡量误差的代价函数，然后寻找使这个代价最小的模型参数。

## **6. 梯度下降**

**梯度下降（Gradient Descent）**是一种常用的优化算法，用于通过迭代来最小化代价函数。其基本思想是：在当前参数的基础上，沿着**代价函数下降最快的方向**（即梯度的反方向）迈出一小步，不断重复这个过程，直到代价函数收敛到最小值。

更形象地说，可以把代价函数想象成一座山，我们站在山上的某一点（初始参数），想要到达山脚的最低点。梯度下降就好比我们在每一步都选择当前最陡的下坡方向走一小步，这样一步步逼近最低点。

梯度下降的数学表示是：

$θ_j := θ_j - α * ∂J(θ)/∂θ_j$

其中，$α$称为**学习率（Learning Rate）**，决定了每一步跨出的“步长”；$∂J(θ)/∂θ_j$是代价函数$J(θ)$对参数$θ_j$的偏导数，也就是代价函数在当前参数处的梯度。负号表示沿着梯度的反方向更新参数，这样可以使代价函数减小。

**关键要点**：

- **学习率**：$α$必须选择合适。如果**太小**，梯度下降会收敛得很慢，需要很多步才能到达最低点；如果**太大**，可能会越过最低点，甚至导致不收敛（发散）。
- **同步更新**：在每次迭代中，所有参数的更新应该**同时进行**。也就是说，计算出的新参数值要基于上一轮的旧参数，不能边更新边代入计算，否则会导致错误。
- **凸函数**：对于线性回归的代价函数（均方误差），其形状是一个**凸函数**（碗状，只有一个全局最小值，没有局部极小值）。因此，无论从哪里开始迭代，梯度下降最终都会收敛到全局最小值。

**示例**：在单变量线性回归中，初始时我们可能将$θ₀$和$θ₁$都初始化为0。然后反复应用梯度下降公式更新$θ₀$和$θ₁$。随着迭代次数增加，$J(θ)$会逐渐减小，最终收敛到最小值附近。此时得到的参数就是使模型最佳拟合数据的参数。

梯度下降是机器学习中非常通用的优化方法，不仅用于线性回归，也广泛用于神经网络等复杂模型的训练。其核心在于**通过迭代逐步逼近最优解**，而不需要直接求解复杂的方程。

## **7. 线性回归的梯度下降**

将梯度下降应用到线性回归的代价函数（均方误差）上，可以推导出具体的参数更新规则。对于线性回归模型$h_θ(x) = θ₀ + θ₁x$（单变量情况），其代价函数为：

$J(θ₀, θ₁) = (1/(2m)) * Σ (θ₀ + θ₁x^i - y^i)^2$

要最小化$J(θ₀, θ₁)$，我们对$θ₀$和$θ₁$分别求偏导，并代入梯度下降公式：

- $θ₀ := θ₀ - α * (1/m) * Σ (h_θ(x^i) - y^i)$
- $θ₁ := θ₁ - α * (1/m) * Σ (h_θ(x^i) - y^i) * x^i$

其中，求和符号$Σ$表示对所有m个训练样本进行求和。上述更新规则需要**同时更新**$θ₀$和$θ₁$（用旧值计算所有新值后再一起更新）。

**直观理解**：每次迭代中，参数更新量与**预测误差**和**特征值**相关。例如，$θ₁$的更新量是学习率乘以平均误差乘以对应的特征值$x^i$。这样，如果预测值比真实值大（误差为正），$θ₁$会减小；反之则增大，从而逐步纠正预测偏差。

由于线性回归的代价函数是凸函数，无论初始参数如何选择，梯度下降最终都会收敛到全局最优的参数，得到一条**最佳拟合直线**。当收敛时，我们得到的直线能够最好地反映输入特征和输出之间的线性关系。

需要注意的是，上述梯度下降每次迭代都会使用**所有训练样本**来计算梯度，因此称为**批量梯度下降（Batch Gradient Descent）**。它保证了收敛性，但在大数据集上计算梯度会比较耗时。后续课程中会介绍更高效的随机梯度下降和小批量梯度下降，它们在每次迭代中只使用部分数据来更新参数，从而加快训练速度。

## **8. 特征缩放**

**特征缩放（Feature Scaling）**是指将不同取值范围的特征调整到相近的尺度上，以便提高机器学习算法的性能和收敛速度。线性回归模型本身对特征尺度并不敏感，因为它会为每个特征学习相应的权重系数。但是，在使用梯度下降等迭代优化算法时，特征尺度不一致会导致**收敛变慢**或需要更精细地调整学习率。

例如，假设我们有两个特征：一个特征取值范围在0到100之间，另一个特征取值范围在0到1之间。如果不做缩放，代价函数的等高线图会是一个拉长的椭圆形状，梯度下降可能需要走很多“之”字形才能收敛到最小值。而如果将两个特征都缩放到相同范围（比如0到1或均值为0、方差为1），代价函数的等高线会更接近圆形，梯度下降可以更直接地朝着最低点前进，从而**加速收敛**。

常用的特征缩放方法有两种：

- **归一化（Normalization）**：将特征缩放到[0,1]区间。公式为：$x'; = (x - x_min)/(x_max - x_min)$。其中$x_min$和$x_max$是该特征的最小值和最大值。
- **标准化（Standardization）**：将特征变换为均值为0、标准差为1的分布。公式为：$x' = (x - μ)/σ$。其中$μ$是均值，$σ$是标准差。

对于梯度下降来说，**标准化**往往更常用，因为它不要求特征值一定在某个范围内，而且对异常值不敏感（归一化的极端值可能导致缩放后的值不稳定）。

**何时需要特征缩放？**一般来说，当使用迭代优化算法（如梯度下降）时，建议对特征进行缩放。如果使用**正规方程**等直接求解的方法，则不需要特征缩放。此外，如果模型使用了正则化，特征缩放会影响正则项的效果，因此更需要先进行缩放。

总之，特征缩放不是线性回归模型本身的要求，但它可以**提高训练效率和稳定性**，让梯度下降更快收敛。在实际操作中，通常会将每个特征都缩放到相近的量级（例如均值为0，标准差1）作为预处理步骤。

## **9. 多项式回归**

**多项式回归（Polynomial Regression）**是线性回归的一种扩展，它允许模型拟合**非线性关系**。其基本思想是在特征中加入**多项式项**，从而将模型从线性形式扩展为多项式形式。虽然模型在参数上仍然是线性的，但通过引入高次特征，可以拟合曲线关系。

例如，对于单变量回归问题，我们可以引入二次项或三次项：

- 二次多项式模型：$h_θ(x) = θ₀ + θ₁x + θ₂x²$
- 三次多项式模型：$h_θ(x) = θ₀ + θ₁x + θ₂x² + θ₃x³$

这样，模型就能拟合抛物线或更复杂的曲线形状，而不仅仅是直线。这在数据呈现非线性趋势时非常有用。例如，房价可能不仅与面积线性相关，也可能与面积的平方相关（价格增长随面积增大而变缓或变快）。

需要注意的是，多项式回归仍然属于**线性模型**的范畴，因为它对参数$θ$来说是线性的。我们只是将原始特征扩展成了多项式特征，然后使用线性回归的方法来拟合。因此，线性回归的所有方法（梯度下降、正规方程等）都可以直接应用。

**特征工程**：在多项式回归中，我们实际上是在进行特征工程——根据原始特征创造新的特征（如$x², x³$）。这样做可以增加模型的复杂度，使其能够捕捉数据中的非线性模式。但同时也可能导致**过拟合**（模型过于复杂，对训练数据拟合很好但对新数据泛化差）。因此，选择合适的多项式阶数很重要，通常需要结合交叉验证等方法来确定。

另外，引入高次项时**特征缩放**尤为重要，因为高次项的数值可能远大于原始特征值（例如x在10左右时，x³就是1000）。如果不进行缩放，不同特征的尺度差异会非常大，不利于梯度下降优化。

总之，多项式回归通过增加特征的次数，让线性模型能够拟合非线性关系，是一种简单而有效的扩展方法。在吴恩达课程的后续部分，还会讨论更复杂的非线性模型以及如何避免过拟合的问题。

## **10. 正规方程**

**正规方程（Normal Equation）**是一种在线性回归中直接求解最优参数的方法。与梯度下降通过迭代优化不同，正规方程提供了一个**闭式解**（closed-form solution），即通过数学公式直接计算出使得代价函数最小的参数值。

对于线性回归模型，假设我们把训练数据表示为矩阵$X$（每一行是一个样本，包括添加的$x₀=1$列以对应偏置项），输出为向量$y$。那么正规方程给出的最优参数$θ$为：

$θ = (X^T X)^{-1} X^T y$

其中$X^T$表示矩阵X的转置，$(X^T X)^{-1}$是矩阵X^T X的逆矩阵。

**求解过程**：正规方程实际上是通过对代价函数求导并令导数为零，解方程组得到参数的最优解。这一过程不需要迭代，直接代数运算即可完成。因此，它在小数据集上计算非常高效，一步到位得到结果。

**与梯度下降的比较**：

- **梯度下降**：需要选择学习率和迭代次数，是一种迭代优化方法，适用于各种规模的数据集（尤其是非常大的数据集，因为梯度下降可以每次用部分数据迭代，内存需求小）。梯度下降需要多次迭代才能收敛，但可以处理特征数量非常大的情况。
- **正规方程**：不需要学习率，也不需要迭代，直接计算结果，实现简单。但计算$(X^T X)^{-1}$的时间复杂度约为O(n³)，其中n是特征数量。因此当特征数量很大时，正规方程的计算会变得非常慢。通常在特征数较少（比如n<10^4）时可以考虑用正规方程，否则还是使用梯度下降更高效。

**数值稳定性**：在实现正规方程时，需要注意$X^T X$可能不可逆的情况（即矩阵奇异）。常见原因包括特征之间**高度相关**（多重共线性）或特征数量多于样本数量。此时可以通过在$X^T X$上加一个小的正则项来保证可逆，或者删除冗余特征。在吴恩达的课程中，也提到了可以使用**伪逆**（Moore-Penrose逆）来处理不可逆的情况，这种方法在数值计算中更为稳定。

**总结**：正规方程为线性回归提供了一个直接求解的途径，在小数据集上非常方便。但在实际机器学习任务中，由于数据集往往很大或特征很多，梯度下降等迭代方法更为常用。不过，理解正规方程有助于我们从数学上把握线性回归的最优解形式，也是后续学习更复杂模型（如岭回归，其解为正规方程加上正则项）的基础。

## **11. 逻辑回归**

**逻辑回归（Logistic Regression）**是一种用于解决**分类问题**的机器学习方法。虽然名字里有“回归”，但它实际上是分类算法。逻辑回归通过拟合一个**逻辑函数（Sigmoid函数）**来预测样本属于某一类别的概率，从而将线性回归的输出映射到[0,1]区间，以表示概率。

逻辑回归的假设函数为：

$h_θ(x) = g(θ^T x) = 1 / (1 + e^{-θ^T x})$

其中$g(z)$是Sigmoid函数，当$z$趋近于正无穷时，$g(z)$趋近于1；当$z$趋近于负无穷时，$g(z)$趋近于0。因此，$h_θ(x)$表示给定特征x时，输出y=1的概率，即$P(y=1 | x; θ)$。

对于二分类问题，我们通常设定一个阈值（如0.5），如果$h_θ(x) ≥ 0.5$则预测为正类（y=1），否则预测为负类（y=0）。对应的决策边界由$θ^T x = 0$决定，即Sigmoid函数的输入为0时对应的点。决策边界可以是线性的，也可以通过引入多项式特征实现非线性的决策边界。

**代价函数**：由于逻辑回归的输出是概率，我们不能直接使用线性回归的均方误差作为代价函数（否则代价函数会出现多个局部极小值，不便于优化）。逻辑回归常用的是**对数损失（Log Loss）**作为代价函数。对于单个样本，其代价函数为：

- 如果真实标签y=1，则代价为 $log(h_θ(x))$；
- 如果真实标签y=0，则代价为 $log(1 - h_θ(x))$。

这一设计使得当预测值与真实值接近时代价很小，而当预测值与真实值偏离很大时代价趋向于无穷大，从而有效惩罚错误的预测。将所有样本的代价平均，就得到整体的代价函数$J(θ)$。

**优化**：逻辑回归的代价函数是凸函数，因此可以使用梯度下降等方法找到全局最小值。通过对$J(θ)$求导，可以得到梯度下降的更新规则。有趣的是，逻辑回归梯度下降的参数更新公式在形式上与线性回归类似，但其中的$h_θ(x)$是Sigmoid函数的输出而非线性函数输出。实践中，我们也可以使用更高级的优化算法（如BFGS、L-BFGS、共轭梯度等）来更快地收敛。

**多分类扩展**：逻辑回归本身用于二分类，但可以通过**一对多（One-vs-Rest）**的方法扩展到多分类问题。例如，对于K个类别，可以训练K个二分类器，每个分类器预测样本是否属于某一个特定类别，然后取概率最大的类别作为最终预测。

总之，逻辑回归是一种简单而高效的分类模型，它在工业界和学术界都有广泛应用。其优点是输出具有概率意义，模型易于理解和快速训练，缺点是只能直接处理线性可分的问题（不过可以通过特征变换处理非线性情况）。

## **12. 正则化**

**正则化（Regularization）**是一种用于防止模型**过拟合**（overfitting）的技术。过拟合是指模型在训练数据上表现很好，但对新数据（测试数据）的泛化能力很差，原因是模型把训练数据中的噪声和细节也学了进去，导致模型过于复杂。正则化通过在代价函数中添加一个**惩罚项**来限制模型参数的大小，从而降低模型的复杂度，提高对未知数据的预测能力。

在线性回归和逻辑回归中，最常用的正则化有两种：

- **L2正则化（岭回归）**：在代价函数中加入参数的L2范数的平方作为惩罚项。例如，线性回归的正则化代价函数变为：$J(θ) = (1/(2m))Σ(h_θ(x^i)-y^i)^2 + (λ/(2m))Σθ_j^2$（j从1到n）。其中$λ$是正则化参数，控制惩罚的力度。L2正则化倾向于让参数取值**更小更平滑**，但不会将参数置零，因此不会减少特征数量。
- **L1正则化（Lasso回归）**：在代价函数中加入参数的L1范数作为惩罚项，即$(λ/(2m))Σ|θ_j|$。L1正则化的一个特点是**倾向于产生稀疏解**，即很多参数会被置为0，相当于自动进行特征选择，保留重要特征，剔除不重要的特征。

**正则化的作用**：通过惩罚大的参数值，正则化迫使模型在“拟合训练数据”和“保持参数小”之间取得平衡。参数越小，模型越简单，对噪声的敏感性越低，从而减少过拟合的风险。需要注意的是，$λ$的选择很关键：如果$λ$太大，惩罚过强，模型可能无法很好地拟合训练数据（导致欠拟合）；如果$λ$太小，起不到正则化效果，可能还是会过拟合。

在吴恩达课程中，正则化不仅用于线性和逻辑回归，也是深度学习中防止过拟合的重要手段（如在神经网络的权重上施加L2正则，称为权重衰减）。此外，课程还提到可以通过**增加训练数据**、**减少特征数量**、**早停法（early stopping）**等方法来缓解过拟合，但正则化是其中应用最广泛的方法之一。

总之，正则化通过在模型复杂度和训练误差之间取得平衡，提高了模型的泛化能力。在实际应用中，我们经常会在代价函数中加入正则项，并通过交叉验证选择合适的正则化强度$λ$，以得到一个既拟合良好又不过度复杂的模型。

## **13. 模型评估指标**

在训练完一个机器学习模型后，我们需要评估它的性能如何。对于**回归问题**和**分类问题**，有不同的常用评估指标：

### **回归问题的评估指标：**

- **均方误差（Mean Squared Error, MSE）**：计算预测值与真实值之差的平方的平均值。MSE越小，模型预测越准确。公式为：$MSE = (1/m)Σ(h_θ(x^i) - y^i)^2$。
- **均方根误差（Root Mean Squared Error, RMSE）**：MSE的平方根，和原始输出的量纲相同，更直观地反映平均误差大小。
- **平均绝对误差（Mean Absolute Error, MAE）**：计算预测误差绝对值的平均值，对异常值不如MSE敏感。公式为：$MAE = (1/m)Σ|h_θ(x^i) - y^i|$。
- **R²系数（决定系数）**：表示模型对数据**变异的解释程度**，取值在0到1之间。R²越接近1，说明模型预测能力越强。R²的计算基于总平方和和残差平方和：$R² = 1 - (残差平方和)/(总平方和)$。其中总平方和是真实值与均值之差的平方和，残差平方和是预测值与真实值之差的平方和。R²=1表示模型完美预测，R²=0表示模型和简单取均值预测一样差。

### **分类问题的评估指标：**

- **准确率（Accuracy）**：分类正确的样本数占总样本数的比例。这是最直观的指标，但在类别不平衡时可能不能很好地反映模型性能。
- **精确率（Precision）**：针对某一类别的指标，表示预测为该类的样本中实际属于该类的比例（即“查准率”）。
- **召回率（Recall）**：针对某一类别的指标，表示实际属于该类的样本中被正确预测的比例（即“查全率”）。
- **F1分数（F1 Score）**：精确率和召回率的调和平均值，用于综合衡量两者。当需要在精确率和召回率之间取得平衡时，F1分数是一个很好的指标。
- **ROC曲线与AUC**：ROC曲线（受试者工作特征曲线）绘制的是真阳性率（召回率）与假阳性率随分类阈值变化的曲线。AUC（Area Under Curve）是ROC曲线下的面积，取值在0.5到1之间。AUC越大，模型的分类性能越好。

**选择指标的依据**：不同的任务关注的指标可能不同。例如，在垃圾邮件分类中，我们可能更关心精确率（避免将正常邮件误判为垃圾邮件）或召回率（尽可能找出所有垃圾邮件），需要根据业务需求权衡。对于回归问题，MSE/RMSE和R²是最常用的指标，其中R²可以直接告诉我们模型解释了多少百分比的方差。

吴恩达课程中强调，在评估模型时，应使用**独立的测试集**或通过**交叉验证**来获取可靠的性能指标。训练误差低并不代表模型泛化能力好，我们需要在未见过的数据上验证模型的表现。通过合理选择评估指标并监控它们，我们可以比较不同模型的优劣，以及判断模型是否存在过拟合或欠拟合的问题。

## **14. 模型选择**

**模型选择（Model Selection）**指的是在多个可能的模型或模型配置中选择最适合当前任务的一个。在机器学习中，我们经常需要比较不同算法或者同一算法的不同参数设置，以决定哪一个能产生最好的预测性能。

模型选择的一个常见做法是使用**训练集-验证集-测试集**的划分：将数据分为三部分，训练集用于训练模型，验证集（开发集）用于评估和选择模型，测试集用于最终评估选定模型的性能。具体步骤如下：

1. **训练集**：用于训练各种候选模型，得到各自的参数。
2. **验证集**：用训练好的模型在验证集上计算性能指标（如分类准确率或回归的MSE）。通过比较这些指标，我们可以选出表现最好的模型。
3. **测试集**：在选定模型后，用测试集来评估该模型的泛化能力。测试集应在模型选择完成后再使用，以避免对测试集过拟合。

如果数据量有限，也可以采用**交叉验证（Cross-Validation）**的方法来替代单独的验证集。例如，k折交叉验证将数据分成k个子集，依次用其中k-1个子集训练模型，用剩下的1个子集验证性能，最后将k次的验证结果取平均作为模型性能的估计。交叉验证能更充分地利用数据，减少单次随机划分带来的偏差。

**选择标准**：在比较模型时，通常以**验证集上的性能**为主要依据。例如，如果有两个模型A和B，A在验证集上的准确率为85%，B为82%，则我们倾向于选择模型A。此外，还需要考虑模型的复杂度和训练速度等因素。如果两个模型性能相近，通常选择较简单的那个（奥卡姆剃刀原则），因为简单模型更不容易过拟合，且计算效率更高。

模型选择贯穿于机器学习项目的始终。例如，我们可能需要选择不同的算法（线性回归 vs. 决策树 vs. SVM等），或者同一算法的不同超参数（如正则化参数`λ`、多项式阶数、神经网络层数等）。吴恩达课程中也介绍了**学习曲线**等工具，通过观察训练误差和验证误差随训练样本数量或迭代次数的变化趋势，来判断模型是欠拟合还是过拟合，从而指导我们调整模型复杂度或增加数据等。

总之，模型选择的目标是在**偏差（欠拟合）**和**方差（过拟合）**之间找到平衡，选出一个在未见数据上表现最佳的模型。合理的模型选择流程（如使用验证集或交叉验证）是保证最终模型性能的关键步骤之一。