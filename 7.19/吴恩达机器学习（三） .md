# 吴恩达机器学习（三）

# 手写数字识别详解

## 一、实验概述

本实验使用神经网络实现手写数字识别系统，包含两个核心部分：

1. **向量化逻辑回归**（多分类问题）
2. **神经网络前向传播与预测**

数据集包含5000个手写数字样本（0-9），每个样本为20×20像素的灰度图像，展开为400维向量。

---

## 二、数据集详解

### 1. 数据结构

- **特征矩阵 X**：5000×400矩阵
    - 每行代表一个样本（手写数字图像）
    - 每列代表一个像素点的灰度值（0-1范围）
- **标签向量 y**：5000维向量
    - 标签编码规则：
        - 数字"1"→"9"：直接对应标签1→9
        - 数字"0"：特殊标记为标签10（适应MATLAB索引从1开始）

### 2. 数据加载

```matlab
load('ex3data1.mat'); % 自动加载X和y矩阵
```

---

## 三、向量化逻辑回归（多分类）

### 1. One-vs-All策略

- **核心思想**：训练10个独立的二分类器（每个对应一个数字）
- **每个分类器任务**：
    - 正样本：目标数字（如"5"）
    - 负样本：其他所有数字（非"5"）

### 2. 正则化代价函数

$J(\theta) = \frac{1}{m}\sum_{i=1}^{m} \left[ -y^{(i)}\log(h_\theta(x^{(i)})) - (1-y^{(i)})\log(1-h_\theta(x^{(i)})) \right] + \frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2$

- **参数说明**：
    - $m$：样本数量（5000）
    - $n$：特征数量（400）
    - $\lambda$：正则化系数（控制过拟合）
    - $h_\theta(x) = g(\theta^Tx)$：Sigmoid函数 $g(z)=\frac{1}{1+e^{-z}}$

### 3. 向量化实现关键

### (1) 假设函数向量化

```matlab
% 同时计算所有样本的预测值
z = X * theta;        % X:5000×401, theta:401×1 → z:5000×1
h = 1./(1+exp(-z));   % Sigmoid函数向量化计算
```

### (2) 代价计算向量化

```matlab
J = (1/m) * sum(-y.*log(h) - (1-y).*log(1-h)); % 核心代价部分
reg_term = (lambda/(2*m)) * sum(theta(2:end).^2); % 正则化项（排除θ₀）
J = J + reg_term; % 最终代价
```

### (3) 梯度计算向量化

$\nabla J(\theta) = \frac{1}{m} X^T (h - y) + \frac{\lambda}{m} \begin{bmatrix} 0 \\ \theta_1 \\ \vdots \\ \theta_n \end{bmatrix}$

```matlab
grad = (1/m) * X' * (h - y); % 基础梯度
grad_reg = (lambda/m) * theta; % 正则化梯度
grad_reg(1) = 0; % 排除θ₀的正则化
grad = grad + grad_reg; % 最终梯度
```

---

## 四、神经网络前向传播

### 1. 网络结构

```
输入层 (400节点) → 隐藏层 (25节点) → 输出层 (10节点)
```

- **预训练参数**：
    - `Theta1`：25×401矩阵（输入层→隐藏层）
        
        ![](https://ima-share-kb.image.myqcloud.com/5/SzEZBCcBfjylJ11fwa1cwH/3bd5fa70-8650-44c0-93e8-e1cede644374.png?sign=37419011ae8677be73ba4315a9e700fe&t=1752927720)
        
    - `Theta2`：10×26矩阵（隐藏层→输出层）

### 2. 前向传播步骤

### (1) 输入层（Layer 1）

$a^{(1)} = \begin{bmatrix} 1 \\ x_1 \\ \vdots \\ x_{400} \end{bmatrix} \quad (\text{添加偏置单元 } a_0^{(1)}=1)$

### (2) 隐藏层（Layer 2）

$z^{(2)} = \Theta^{(1)} a^{(1)}$

### (3) 输出层（Layer 3）

$z^{(3)} = \Theta^{(2)} a^{(2)}$

### 3. 向量化实现（MATLAB）

```matlab
% 添加偏置单元到输入层
a1 = [ones(m,1) X]; % 5000×401

% 隐藏层计算
z2 = a1 * Theta1'; % 5000×25
a2 = [ones(m,1) sigmoid(z2)]; % 5000×26

% 输出层计算
z3 = a2 * Theta2'; % 5000×10
a3 = sigmoid(z3); % 5000×10

% 预测（取最大概率索引）
[~, pred] = max(a3, [], 2);
```

### 4. 预测结果处理

- **索引映射规则**：
    - 索引1→9：对应数字1→9
    - 索引10：对应数字0
- **准确率**：约97.5%（使用预训练参数）

---

## 五、关键概念详解

### 1. Sigmoid激活函数

$g(z) = \frac{1}{1+e^{-z}}$

- **特性**：
    - 将实数映射到(0,1)区间
    - 导数计算高效：$g'(z) = g(z)(1-g(z))$
    - 适合概率输出

### 2. 向量化编程优势

| 方法 | 计算时间 | 代码复杂度 |
| --- | --- | --- |
| 循环迭代 | O(mn) | 高（需嵌套循环） |
| 矩阵运算 | O(1) | 低（单行代码） |

### 3. 正则化作用

- **解决过拟合**：惩罚大权重值
- **数学表达**：
$\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2$
- **实现技巧**：
    - 排除偏置项θ₀的正则化
    - λ过大导致欠拟合，λ过小导致过拟合

### 4. One-vs-All策略

```
原始问题（10分类）
├── 分类器1：1 vs 非1
├── 分类器2：2 vs 非2
├── ...
└── 分类器10：0 vs 非0
```

---

## 六、实验实现建议

1. **维度检查**：
    
    ```matlab
    % 关键矩阵维度验证
    size(X)        % 应返回 5000 400
    size(Theta1)   % 应返回 25 401
    size(Theta2)   % 应返回 10 26
    ```
    
2. **梯度检验**：
    - 比较向量化梯度与数值梯度（差分法）
    - 差异应小于10⁻⁹
3. **可视化调试**：
    
    ```matlab
    % 显示随机样本
    rp = randperm(m);
    for i = 1:100
      displayData(X(rp(i), :));
      pred = predict(Theta1, Theta2, X(rp(i),:));
      fprintf('Neural Network Prediction: %d\n', mod(pred,10));
    end
    ```
    

---

> 实验重点：理解从逻辑回归到神经网络的演进过程，掌握向量化实现的核心技巧，体会正则化对模型泛化能力的影响。实际编程时需特别注意矩阵维度匹配问题。
>